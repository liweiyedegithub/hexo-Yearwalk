---
title: 全球新闻聚合+飞书通知
date: 2026-1-13 10:25:01
tags: 
  - News
  - RSS
top: false
categories: 
  - 飞书机器人
img: /medias/featureimages/13.jpg

---
```
# 从 0 到 1：RSS 全球新闻聚合 → 腾讯云翻译 → SQLite 入库 → 飞书机器人分发（Ubuntu 定时任务版）

> 目标：每天早上通勤前（8:30 前）自动获取过去 24 小时的全球重点新闻（含地区/人物专题），翻译成中文，入库到 SQLite，并按地区/人物分别推送到对应的飞书机器人群里。  
> 环境：Ubuntu + Python（venv）+ 本地代理（HTTP 127.0.0.1:7897）+ cron 定时任务。

---

## 1. 整体架构与数据流

这套系统由四块组成：

1) **采集层（RSS）**  
从多个新闻源拉取 RSS（例如 BBC、Reuters、Al Jazeera 等），并做“时间范围 + 关键词”过滤，得到候选新闻列表。

2) **翻译层（腾讯云 TMT）**  
对标题与正文做中文翻译，并加入缓存/失败回退。为了复用能力，把翻译能力独立成 CLI 工具脚本（可单独调用）。

3) **存储层（SQLite）**  
所有新闻统一写入一个 `news` 表，通过 `region_key` / `region_name` 标识地区或专题（人物），通过 `url` 去重，记录是否已发送飞书以及发送时间。

4) **分发层（飞书机器人）**  
按 `region_key` 将新闻分发到不同机器人；“人物专题”多个 key 也可以路由到同一个机器人。消息格式调整为结构化：标题（可点击跳转原文）、正文、来源。

最终由 **cron** 每天一次执行：先抓取入库，再发送分发，保证 8:30 前推送完成。

---

## 2. 前期准备

### 2.1 代理与网络策略
新闻源访问存在网络限制，因此最终使用 **本地 HTTP 代理**解决（端口为 `127.0.0.1:7897`）。

- 关键结论：  
  - 系统默认路由并未被 VPN 接管（`ip route` 默认走网卡 `wlo1`），直连会超时。  
  - 使用 `curl -x http://127.0.0.1:7897 URL` 可成功访问，证明代理有效。
- 方案选择：  
  - 不改系统路由  
  - 统一通过环境变量 `http_proxy/https_proxy/all_proxy` 让脚本走代理

### 2.2 Python 包管理策略（PEP 668）
Ubuntu 上使用系统 Python 时，遇到 `externally-managed-environment`（PEP 668），禁止全局 pip 安装。

- 最终选择：使用 **venv 虚拟环境**，避免污染系统 Python。
- 并将 venv 统一放到 `/app/python_scripts/.venv`，方便未来其它脚本复用同一套运行环境。

---

## 3. 环境检查与依赖安装

### 3.1 环境检查脚本
编写了一个环境检查脚本，用于验证：
- Python / pip / venv 是否可用
- RSS 源是否可访问
- SQLite 是否可用
- 项目依赖是否安装（例如 `feedparser`、`tencentcloud-sdk-python`）
- 项目文件是否齐全（配置文件、采集脚本、发送脚本、数据库工具等）

目的：在迁移到 Ubuntu 后能快速发现缺失项，并给出修复路径。

### 3.2 venv 创建与依赖安装
在 `/app/python_scripts/.venv` 中安装核心依赖：
- `feedparser`（解析 RSS）
- `tencentcloud-sdk-python`（腾讯云机器翻译 SDK）

后续所有定时任务都通过该 venv 的 python 执行。

---

## 4. 配置设计（config.json）

配置文件集中管理“采集策略、地区/人物专题、飞书路由、数据库路径”等信息，核心思想是：
- **新增地区/人物专题只改 config，不改代码**
- **不同专题可绑定不同机器人，也可多个专题共用同一个机器人**

### 4.1 profiles：地区/专题规则
每个 profile 包含：
- `key`：唯一标识（例如 `middle_east`、`russia`、`southeast_asia`、`person_musk`）
- `name`：展示名称（推送消息中用于标识）
- `hours`：时间窗口（默认 24 小时，可按需调整）
- `per_source/max_items`：抓取量与上限
- `keywords_any`：命中关键词列表（用于筛选新闻属于哪个 profile）

地区更正：将“中南亚”配置更正为“东南亚”，并同步更新 profile key 与飞书路由 key，避免数据分发错位。

### 4.2 feishu.routes：专题 → webhook 映射
按 `region_key` 找到对应机器人 webhook：
- 地区：各自独立机器人
- 人物专题：多个 `person_*` 可以共用同一个机器人 webhook（实现“人物频道”）

---

## 5. 采集脚本改造要点（getNew_multi.py）

### 5.1 RSS 请求超时与失败隔离
增加 RSS 访问超时机制：
- 单个源超过 30 秒自动放弃
- 继续处理下一个源（避免卡死）
- 汇总打印“失败源列表”，便于排查

实现策略：
- 不直接让 feedparser 联网
- 先使用标准库拉取 RSS 内容（带 timeout）
- 再把原始内容交给 feedparser 解析

### 5.2 代理接入策略
由于系统不走全局 VPN，脚本通过环境变量方式使用代理：
- `http_proxy=http://127.0.0.1:7897`
- `https_proxy=http://127.0.0.1:7897`
- `all_proxy=http://127.0.0.1:7897`

### 5.3 数据入库策略（单表）
明确选择“一个表存全部内容”，通过字段区分地区/专题：
- `region_key` / `region_name`：地区/人物专题标识
- `url` 作为去重依据（通常每条新闻唯一）
- `sent_feishu` 与 `sent_feishu_at` 控制发送状态

---

## 6. 飞书发送脚本改造要点（feishu_sender_multi.py）

### 6.1 分发模型
- 读取 `news.db` 中 `sent_feishu=0` 的记录
- 按 `region_key` 分组
- 根据 `feishu.routes` 找到 webhook
- **每条新闻单独发送一条消息**（避免合并导致混乱）
- 发送成功才更新数据库标记，确保幂等与可重试

### 6.2 消息格式优化（结构化 + 标题可点击）
将消息格式调整为更清晰的结构，避免“正文与链接混乱”：
- 标题：可点击跳转原文 URL
- 正文：中文摘要/正文（可截断避免过长）
- 来源：新闻源名称

这样在飞书里阅读体验更接近“每日新闻简报”。

---

## 7. 数据库工具脚本（db_tools.py）

为了便于维护与回溯，提供数据库工具脚本实现：
- `stats`：查看 news 表行数、表占用（优先 dbstat，否则估算）、数据库文件大小
- `clear`：清空表并可选 VACUUM 收缩
- `reset_all`：全部改为“未发送”（重新推送场景）
- `reset_recent [hours]`：最近 N 小时改为“未发送”（默认 24 小时）

用途：
- 测试推送时可快速“重置状态”
- 每次改消息格式后可对最近一段进行重发验证

---

## 8. 定时任务（cron）与运行封装

### 8.1 统一 env.sh（复用 venv + 代理 + 编码）
将运行环境统一封装在 `/app/python_scripts/env.sh`：
- 指向统一 venv `/app/python_scripts/.venv`
- 设置代理 `127.0.0.1:7897`
- 设置 UTF-8 编码（避免 cron 环境下乱码）

任何脚本只需 `source /app/python_scripts/env.sh` 即可稳定运行。

### 8.2 run_fetch.sh / run_send.sh（项目级入口）
为 news 项目创建运行入口：
- 切换到项目目录
- 先做日志轮转（按月）
- 在日志中写入“本次开始时间/结束时间/返回码”
- 再执行采集或发送脚本

### 8.3 日志按月切割（每月 1 号归档到 logs/）
实现“按月切割”策略：
- 当前月日志固定写入 `log_fetch.log` / `log_send.log`
- 每月首次运行时归档上月到 `logs/`，文件名包含年月
- 归档与写入过程不依赖 cron，脚本自维护轮转状态

### 8.4 cron 调度（每天一次，保证 8:30 前完成）
将频率从“每 15 分钟”调整为“每天一次”：
- 08:20 抓取翻译入库
- 08:25 发送飞书（或更稳妥 08:28）

示例：
- `20 8 * * * run_fetch.sh`
- `25 8 * * * run_send.sh`

目标：上班通勤前即可在飞书看到当天的“全球/地区/人物”重点新闻推送。

---

## 9. 结果校验与验收方式

### 9.1 网络校验
- 直连 BBC RSS 超时是正常现象（网络限制）
- 使用代理 `curl -x http://127.0.0.1:7897 URL` 返回 HTTP 200 表示代理可用
- 脚本运行前通过 env.sh 注入代理确保一致性

### 9.2 数据校验
- 查看 `news` 表是否持续增长、是否按 `url` 去重
- 检查 `sent_feishu` 是否在发送成功后正确置为 1
- 如果需要重发，通过 `db_tools.py reset_recent 24` 对最近新闻回放验证

### 9.3 推送校验
- 每条新闻一条消息
- 标题点击可打开原文
- 消息包含：标题/正文/来源/地区(或人物标识)

---

## 10. 收获与可扩展点

- **配置驱动**：地区与人物专题通过 `profiles` 扩展即可，无需改代码。
- **分发灵活**：一个专题一个机器人；或多个专题共用一个机器人（例如人物频道）。
- **稳定运行**：代理环境注入 + RSS 超时控制 + 日志轮转 + cron 调度，让系统可长期运行。
- **可维护性**：数据库工具脚本用于统计、清库、重发，方便迭代消息格式与测试。

后续可扩展方向：
- 引入更多 RSS 源（财经/科技/地区媒体）
- 关键词精细化（正向关键词 + 排除关键词）
- 增加失败重试策略与告警（例如抓取失败推送提醒）
- 用 systemd timer 替代 cron（更强的可观测性和失败恢复）

---

> 至此，整个“每日全球新闻自动简报系统”已经可以在 Ubuntu 上稳定定时运行：  
> 每天 8:20 自动采集并翻译 → 入库 → 8:25 按地区/人物推送飞书，确保通勤路上即可阅读。
```
